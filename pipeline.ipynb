{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ce359eda",
            "metadata": {},
            "source": [
                "### imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "67380bf1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import neo4j\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "# LLM and Embedding Model\n",
                "from neo4j_graphrag.llm import OpenAILLM\n",
                "from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4edfdfbf",
            "metadata": {},
            "source": [
                "### Load env variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "ac7a88c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "NEO4J_URI = os.getenv('NEO4J_URI')\n",
                "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
                "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "fa4dceb2",
            "metadata": {},
            "outputs": [],
            "source": [
                "neo4j_driver = neo4j.GraphDatabase.driver(NEO4J_URI,\n",
                "                auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "33b35f3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm=OpenAILLM(\n",
                "   model_name=\"gpt-4o-mini\",\n",
                "   model_params={\n",
                "       \"response_format\": {\"type\": \"json_object\"}, # use json_object formatting for best results\n",
                "       \"temperature\": 0 # turning temperature down for more deterministic results\n",
                "   }\n",
                ")\n",
                "embedder = OpenAIEmbeddings()\n",
                "\n",
                "# define prompt template (generalized for any document type)\n",
                "prompt_template = '''\n",
                "You are a Knowledge Engineer task with extracting structured information from unstructured text \n",
                "to build a comprehensive property graph for advanced data analysis and retrieval.\n",
                "\n",
                "Extract the entities (nodes) and specify their type from the following Input text based on the provided schema. \n",
                "Also, extract the directed relationships between these nodes.\n",
                "\n",
                "Return the result strictly as a JSON object using the following format:\n",
                "{{\"nodes\": [ {{\"id\": \"unique_id\", \"label\": \"Entity_Type\", \"properties\": {{\"name\": \"Entity Name\" }} }}],\n",
                "  \"relationships\": [{{\"type\": \"RELATIONSHIP_TYPE\", \"start_node_id\": \"unique_id\", \"end_node_id\": \"unique_id\", \"properties\": {{\"details\": \"Brief description of how they interact\"}} }}] }}\n",
                "\n",
                "---\n",
                "\n",
                "### Constraints:\n",
                "1. **Schema Adherence:** Use only the following node labels and relationship types:\n",
                "{schema}\n",
                "\n",
                "2. **Node IDs:** Assign a unique string ID to each node and use these IDs to define the relationships.\n",
                "3. **Directionality:** Ensure the relationship direction (start_node to end_node) reflects the logic of the source text.\n",
                "4. **Format:** Do not include any conversational text, preamble, or markdown formatting outside of the JSON block.\n",
                "\n",
                "---\n",
                "\n",
                "### Examples:\n",
                "{examples}\n",
                "\n",
                "---\n",
                "\n",
                "### Input text:\n",
                "{text}\n",
                "'''"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79280ca2",
            "metadata": {},
            "source": [
                "### Extract texts from documents\n",
                "\n",
                "`DocumentLoader` auto-detects whether a PDF has native text (→ pdfplumber) or is scanned (→ OCR via pytesseract).  \n",
                "Set `force_ocr=True` to force the OCR path on any file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d27110b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Document Metadata ===\n",
                        "  file_path: /Users/karlo/Documents/Wizonix/graph-rag/test-documents/resume-for-ref-4yoe.png\n",
                        "  file_type: image\n",
                        "  page_count: 1\n",
                        "  ocr_used: True\n",
                        "  ocr_confidence: 0.933\n",
                        "\n",
                        "=== Text Preview (3353 total chars) ===\n",
                        "First Lastname\n",
                        "\n",
                        "000-000-0000 - email.com - linkedin.com/in/linkedin - github.com hub\n",
                        "\n",
                        "EXPERIENCE\n",
                        "\n",
                        "Gov Contractor Remote\n",
                        "Lead Full-Stack Software Engineer October 2023 — May 2025\n",
                        "\n",
                        "© Served as front-end lead for a greenfield GIS data analytics and visualization platform for federal clients.\n",
                        "\n",
                        "Designed system architecture and led full-stack implementation for an interactive, low-code data modeling and\n",
                        "analysis workflow builder using custom Angular components and services with TypeScript and Material UI,\n",
                        "Python Flask REST endpoints, and Postgres. Directed and mentored two junior developers remotely.\n",
                        "Implemented SSO authentication and authorization flow using Keycloak and OAuth 2.0, improving user expe-\n",
                        "rience and cutting customer onboarding time by an average of 30 minutes/user.\n",
                        "\n",
                        "Improved data visualization and page performance of tabular and map views by optimizing render of tens of\n",
                        "thousands of data records using lazy loading and prefetching techniques, reducing latency from 8s to under 2s.\n",
                        "Defined and enforced frontend code quality standards and best practices, leading to an 80% drop in post-\n",
                        "deployment bugs across major features. Spearheaded developer process improvements including an automated\n",
                        "CI/CD pipeline using GitHub Actions, Jest, and ESLint; increased test coverage from 47% to 76%.\n",
                        "\n",
                        "Onboarded 4 new hires, reducing average ramp-up time from 4 weeks to 1.5 through improved documentation.\n",
                        "\n",
                        ".\n",
                        "\n",
                        "Amazon Seattle, WA\n",
                        "Software Development Engineer Jul 2021 — Sep 2023\n",
                        "\n",
                        "As part of Alexa Skills Monitoring, built distributed systems to ingest, process, and monitor data from 3rd party\n",
                        "Alexa skills, developers, and devices at 100K+ records/day.\n",
                        "\n",
                        "Designed and built full-stack implementation of new service to automate legal escalations for ambiguous vio-\n",
                        "lations using Java, Spring, React, Javascript, and GraphQL, reducing manual investigator workload by 8,000\n",
                        "hours/year and saving $500K annually.\n",
                        "\n",
                        "Launched feature to detect suspicious silences in skills response audio using AWS Step Functions, Lambda, and\n",
                        "EMR, serving over 10K requests/hour and reducing false positive violation rate by 16%.\n",
                        "\n",
                        "Owned Operational Excellence initiatives to root cause and close high-severity tickets by on-call owner, imple-\n",
                        "menting long-term fixes that reduced incoming high-severity tickets by 37% (6 per month).\n",
                        "\n",
                        ".\n",
                        "\n",
                        "Amazon Seattle, WA\n",
                        "Software Development Engineer Intern Jun 2020 — Aug 2020\n",
                        "\n",
                        "Created Action Ripple Effect monitor, which proactively flags risky skills related to previously non-compliant\n",
                        "skills, eliminating 12+ hours of manual investigation and customer impact per violation.\n",
                        "« Developed backend stack using Java, AWS CloudFormation, Lambda, DynamoDB, and SQS/SNS.\n",
                        "\n",
                        "EDUCATION\n",
                        "\n",
                        "University of School May 2021\n",
                        "B.S. Computer Science 3.8 GPA (Highest Distinction), Dean’s List\n",
                        "\n",
                        "SKILLS\n",
                        "\n",
                        "Languages JavaScript, TypeScript, Java, Python, GraphQL, SQL, HTML, CSS, SASS/SCSS\n",
                        "Technologies React, Redux, Jest, Playwright, Next.js, Express.js, Tailwind CSS, Webpack, Babel, ESLint\n",
                        "Tools Git, Node.js, Docker, AWS (EMR, CloudFormation, CloudWatch, Lambda), Figma\n",
                        "\n",
                        "PROJECTS\n",
                        "\n",
                        "Project name, Python (Flask), JavaScript, PostgreSQL, Heroku\n",
                        "\n",
                        "projectname.herokuapp.com\n",
                        "« In-browser, live-reload text editor for designers to preview, share, and collaborate on HTML/CSS/JS code\n",
                        "Hosted on Heroku and supported by voluntary contributions from over 30+ MAU\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "from doc_to_graphrag.ingestion import DocumentLoader\n",
                "\n",
                "loader = DocumentLoader()\n",
                "\n",
                "# ── Load a document (auto-detects native text vs scanned) ──\n",
                "# Set force_ocr=True to force OCR even on native-text PDFs\n",
                "result = loader.load('test-documents/2PX3WorksheetWeek2_restore.pdf')\n",
                "\n",
                "# ── Metadata ──\n",
                "print('=== Document Metadata ===')\n",
                "for k, v in result['metadata'].items():\n",
                "    print(f'  {k}: {v}')\n",
                "\n",
                "# ── Text preview ──\n",
                "extracted_text = result['text']\n",
                "print(f'\\n=== Text Preview ({len(extracted_text)} total chars) ===')\n",
                "print(extracted_text)\n",
                "print('...')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24bfb998",
            "metadata": {},
            "source": [
                "### infer graph schema and relationships (llm based)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "1ec6e827",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Split into 44 chunks\n",
                        "First chunk preview: Week 2\n",
                        "Overview and Goals\n",
                        "· Introduce the recycling project.\n",
                        "· Construct a rudimentary framework to evaluate and rank proposed designs.\n",
                        "· Introduce unitless cost equations.\n",
                        "· Construct a rudimentary u...\n"
                    ]
                }
            ],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from neo4j_graphrag.experimental.components.text_splitters.langchain import LangChainTextSplitterAdapter\n",
                "\n",
                "langchain_text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=600,       # Smaller chunks = better entity extraction\n",
                "    chunk_overlap=60,     # 10% overlap to keep context\n",
                "    length_function=len,\n",
                "    separators=[\n",
                "            \"\\n\\n\",\n",
                "            \"\\n\",\n",
                "            \" \",\n",
                "            \".\",\n",
                "            \",\",\n",
                "            \"\\u200b\",  # Zero-width space\n",
                "            \"\\uff0c\",  # Fullwidth comma\n",
                "            \"\\u3001\",  # Ideographic comma\n",
                "            \"\\uff0e\",  # Fullwidth full stop\n",
                "            \"\\u3002\",  # Ideographic full stop\n",
                "            \"\",\n",
                "        ],\n",
                "    is_separator_regex=False\n",
                ")\n",
                "\n",
                "adapted_text_splitter = LangChainTextSplitterAdapter(RecursiveCharacterTextSplitter())\n",
                "\n",
                "# Use the extracted text from the document loader above\n",
                "split_texts = langchain_text_splitter.split_text(extracted_text)\n",
                "print(f'Split into {len(split_texts)} chunks')\n",
                "print(f'First chunk preview: {split_texts[0][:200]}...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "384c2929",
            "metadata": {},
            "outputs": [],
            "source": [
                "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
