{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ce359eda",
            "metadata": {},
            "source": [
                "### imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "67380bf1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import neo4j\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "# LLM and Embedding Model\n",
                "from neo4j_graphrag.llm import OpenAILLM\n",
                "from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4edfdfbf",
            "metadata": {},
            "source": [
                "### Load env variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "ac7a88c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "NEO4J_URI = os.getenv('NEO4J_URI')\n",
                "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
                "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "fa4dceb2",
            "metadata": {},
            "outputs": [],
            "source": [
                "neo4j_driver = neo4j.GraphDatabase.driver(NEO4J_URI,\n",
                "                auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "33b35f3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm=OpenAILLM(\n",
                "   model_name=\"gpt-4o-mini\",\n",
                "   model_params={\n",
                "       \"response_format\": {\"type\": \"json_object\"}, # use json_object formatting for best results\n",
                "       \"temperature\": 0 # turning temperature down for more deterministic results\n",
                "   }\n",
                ")\n",
                "embedder = OpenAIEmbeddings()\n",
                "\n",
                "# define prompt template (generalized for any document type)\n",
                "prompt_template = '''\n",
                "You are a Knowledge Engineer task with extracting structured information from unstructured text \n",
                "to build a comprehensive property graph for advanced data analysis and retrieval.\n",
                "\n",
                "Extract the entities (nodes) and specify their type from the following Input text based on the provided schema. \n",
                "Also, extract the directed relationships between these nodes.\n",
                "\n",
                "Return the result strictly as a JSON object using the following format:\n",
                "{{\"nodes\": [ {{\"id\": \"unique_id\", \"label\": \"Entity_Type\", \"properties\": {{\"name\": \"Entity Name\" }} }}],\n",
                "  \"relationships\": [{{\"type\": \"RELATIONSHIP_TYPE\", \"start_node_id\": \"unique_id\", \"end_node_id\": \"unique_id\", \"properties\": {{\"details\": \"Brief description of how they interact\"}} }}] }}\n",
                "\n",
                "---\n",
                "\n",
                "### Constraints:\n",
                "1. **Schema Adherence:** Use only the following node labels and relationship types:\n",
                "{schema}\n",
                "\n",
                "2. **Node IDs:** Assign a unique string ID to each node and use these IDs to define the relationships.\n",
                "3. **Directionality:** Ensure the relationship direction (start_node to end_node) reflects the logic of the source text.\n",
                "4. **Format:** Do not include any conversational text, preamble, or markdown formatting outside of the JSON block.\n",
                "\n",
                "---\n",
                "\n",
                "### Examples:\n",
                "{examples}\n",
                "\n",
                "---\n",
                "\n",
                "### Input text:\n",
                "{text}\n",
                "'''"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79280ca2",
            "metadata": {},
            "source": [
                "### Extract texts from documents\n",
                "\n",
                "`DocumentLoader` auto-detects whether a PDF has native text (→ pdfplumber) or is scanned (→ OCR via pytesseract).  \n",
                "Set `force_ocr=True` to force the OCR path on any file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "6d27110b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Document Metadata ===\n",
                        "  file_path: /Users/karlo/Documents/Wizonix/graph-rag/test-documents/2PX3WorksheetWeek2_restore.pdf\n",
                        "  file_type: pdf\n",
                        "  page_count: 19\n",
                        "  ocr_used: False\n",
                        "  ocr_confidence: None\n",
                        "\n",
                        "=== Text Preview (20180 total chars) ===\n",
                        "Week 2\n",
                        "Overview and Goals\n",
                        "· Introduce the recycling project.\n",
                        "· Construct a rudimentary framework to evaluate and rank proposed designs.\n",
                        "· Introduce unitless cost equations.\n",
                        "· Construct a rudimentary unitless cost equation.\n",
                        "Table of Contents\n",
                        "Overview and Goals........................................................................................................................................i\n",
                        "Table of Figures.......................................................................................\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "from doc_to_graphrag.ingestion import DocumentLoader\n",
                "\n",
                "loader = DocumentLoader()\n",
                "\n",
                "# ── Load a document (auto-detects native text vs scanned) ──\n",
                "# Set force_ocr=True to force OCR even on native-text PDFs\n",
                "result = loader.load('test-documents/2PX3WorksheetWeek2_restore.pdf')\n",
                "\n",
                "# ── Metadata ──\n",
                "print('=== Document Metadata ===')\n",
                "for k, v in result['metadata'].items():\n",
                "    print(f'  {k}: {v}')\n",
                "\n",
                "# ── Text preview ──\n",
                "extracted_text = result['text']\n",
                "print(f'\\n=== Text Preview ({len(extracted_text)} total chars) ===')\n",
                "print(extracted_text[:500])\n",
                "print('...')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24bfb998",
            "metadata": {},
            "source": [
                "### Chunk text (overlapping chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "1ec6e827",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Split into 44 chunks\n",
                        "First chunk preview: Week 2\nOverview and Goals\n· Introduce the recycling project.\n· Construct a rudimentary framework to evaluate and rank proposed designs.\n· Introduce unitless cost equations.\n· Construct a rudimentary u...\n"
                    ]
                }
            ],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from neo4j_graphrag.experimental.components.text_splitters.langchain import LangChainTextSplitterAdapter\n",
                "\n",
                "langchain_text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=600,       # Smaller chunks = better entity extraction\n",
                "    chunk_overlap=60,     # 10% overlap to keep context\n",
                "    length_function=len,\n",
                "    separators=[\n",
                "            \"\\n\\n\",\n",
                "            \"\\n\",\n",
                "            \" \",\n",
                "            \".\",\n",
                "            \",\",\n",
                "            \"\\u200b\",  # Zero-width space\n",
                "            \"\\uff0c\",  # Fullwidth comma\n",
                "            \"\\u3001\",  # Ideographic comma\n",
                "            \"\\uff0e\",  # Fullwidth full stop\n",
                "            \"\\u3002\",  # Ideographic full stop\n",
                "            \"\",\n",
                "        ],\n",
                "    is_separator_regex=False\n",
                ")\n",
                "\n",
                "adapted_text_splitter = LangChainTextSplitterAdapter(RecursiveCharacterTextSplitter())\n",
                "\n",
                "# Use the extracted text from the document loader above\n",
                "split_texts = langchain_text_splitter.split_text(extracted_text)\n",
                "print(f'Split into {len(split_texts)} chunks')\n",
                "print(f'First chunk preview: {split_texts[0][:200]}...')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0b42b41",
            "metadata": {},
            "source": [
                "### Step 1: Infer graph schema (LLM-based)\n",
                "\n",
                "`SchemaInferrer` sends the first few chunks to the LLM to discover what **node labels** (entity types) and **relationship types** are relevant for this document. It then samples later chunks to refine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6adfa8e9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from doc_to_graphrag.extraction import SchemaInferrer\n",
                "\n",
                "inferrer = SchemaInferrer(llm, initial_chunk_count=3, refine_sample_count=5)\n",
                "schema = inferrer.infer(split_texts)\n",
                "\n",
                "print('=== Inferred Schema ===')\n",
                "print(f'Node labels: {schema[\"node_labels\"]}')\n",
                "print(f'Rel types:   {schema[\"rel_types\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa1b2c3d",
            "metadata": {},
            "source": [
                "### Step 2: Build knowledge graph (SimpleKGPipeline)\n",
                "\n",
                "`SimpleKGPipeline` takes the inferred schema and uses the LLM internally to extract entities and relationships from the text, then writes them directly to Neo4j."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb2c3d4e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
                "\n",
                "kg_builder = SimpleKGPipeline(\n",
                "    llm=llm,\n",
                "    driver=neo4j_driver,\n",
                "    embedder=embedder,\n",
                "    entities=schema['node_labels'],        # e.g. [\"Person\", \"Organization\", ...]\n",
                "    relations=schema['rel_types'],          # e.g. [\"FOUNDED\", \"WORKS_FOR\", ...]\n",
                "    from_pdf=False,                         # we already extracted the text\n",
                "    text_splitter=adapted_text_splitter,\n",
                "    perform_entity_resolution=True,\n",
                "    prompt_template=prompt_template,\n",
                ")\n",
                "\n",
                "print('SimpleKGPipeline ready.')\n",
                "print(f'  Entities: {schema[\"node_labels\"]}')\n",
                "print(f'  Relations: {schema[\"rel_types\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc3d4e5f",
            "metadata": {},
            "source": [
                "### Step 3: Run the pipeline\n",
                "\n",
                "This sends the extracted text through the pipeline — the LLM scans each chunk, extracts entities/relationships using the schema, and writes them to Neo4j."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd4e5f6a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the pipeline (async under the hood)\n",
                "kg_result = await kg_builder.run_async(text=extracted_text)\n",
                "\n",
                "print('\\n=== Pipeline Result ===')\n",
                "print(kg_result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}