{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Hybrid Query: Vector + Graph RAG\n",
                "\n",
                "Three query modes following [Vectors and Graphs: Better Together](https://neo4j.com/blog/developer/vectors-graphs-better-together/):\n",
                "\n",
                "1. **Vector RAG** — pure semantic similarity on text chunks\n",
                "2. **Graph RAG** — vector search + graph traversal via `VectorCypherRetriever`\n",
                "3. **Hybrid** — merges context from both retrievers for a comprehensive answer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2c3d4e5",
            "metadata": {},
            "source": [
                "### Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c3d4e5f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import neo4j\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "from neo4j_graphrag.llm import OpenAILLM\n",
                "from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings\n",
                "from neo4j_graphrag.indexes import create_vector_index\n",
                "from neo4j_graphrag.retrievers import VectorRetriever, VectorCypherRetriever\n",
                "from neo4j_graphrag.generation import GraphRAG, RagTemplate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d4e5f6a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "\n",
                "NEO4J_URI = os.getenv('NEO4J_URI')\n",
                "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
                "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
                "\n",
                "neo4j_driver = neo4j.GraphDatabase.driver(NEO4J_URI,\n",
                "                auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
                "\n",
                "embedder = OpenAIEmbeddings()\n",
                "llm = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0.0})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e5f6a7b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create vector index (idempotent — safe to re-run)\n",
                "create_vector_index(neo4j_driver, name=\"text_embeddings\", label=\"Chunk\",\n",
                "                   embedding_property=\"embedding\", dimensions=1536, similarity_fn=\"cosine\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6a7b8c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shared RAG prompt template\n",
                "rag_template = RagTemplate(\n",
                "    template='''Answer the Question using the following Context. Only respond with information mentioned in the Context. Do not inject any speculative information not mentioned.\n",
                "\n",
                "# Question:\n",
                "{query_text}\n",
                "\n",
                "# Context:\n",
                "{context}\n",
                "\n",
                "# Examples:\n",
                "{examples}\n",
                "\n",
                "# Answer:\n",
                "''',\n",
                "    expected_inputs=['query_text', 'context', 'examples']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0b1c2d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query to test\n",
                "q = \"Placeholder.\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "g7b8c9d0",
            "metadata": {},
            "source": [
                "---\n",
                "### Option 1: Vector RAG\n",
                "\n",
                "Pure semantic similarity search over `Chunk` text embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "h8c9d0e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "vector_retriever = VectorRetriever(\n",
                "    neo4j_driver,\n",
                "    index_name=\"text_embeddings\",\n",
                "    embedder=embedder,\n",
                "    return_properties=[\"text\"],\n",
                ")\n",
                "\n",
                "vector_rag = GraphRAG(llm=llm, retriever=vector_retriever, prompt_template=rag_template)\n",
                "\n",
                "vector_answer = vector_rag.search(q, retriever_config={'top_k': 5}).answer\n",
                "print(\"=== Vector RAG ===\")\n",
                "print(vector_answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "i9d0e1f2",
            "metadata": {},
            "source": [
                "---\n",
                "### Option 2: Graph RAG (VectorCypher)\n",
                "\n",
                "Vector search as the entry point, then traverse 1-2 hops through entity relationships\n",
                "to enrich the context with structured graph information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "j0e1f2a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "graph_retriever = VectorCypherRetriever(\n",
                "    neo4j_driver,\n",
                "    index_name=\"text_embeddings\",\n",
                "    embedder=embedder,\n",
                "    retrieval_query=\"\"\"\n",
                "//1) Go out 1-2 hops in the entity graph and get relationships\n",
                "WITH node AS chunk\n",
                "MATCH (chunk)<-[:FROM_CHUNK]-(entity)-[relList:!FROM_CHUNK]-{1,2}(nb)\n",
                "UNWIND relList AS rel\n",
                "\n",
                "//2) collect relationships and text chunks\n",
                "WITH collect(DISTINCT chunk) AS chunks, collect(DISTINCT rel) AS rels\n",
                "\n",
                "//3) format and return context\n",
                "RETURN apoc.text.join([c in chunks | c.text], '\\n') +\n",
                "  apoc.text.join([r in rels |\n",
                "  startNode(r).name+' - '+type(r)+' '+r.details+' -> '+endNode(r).name],\n",
                "  '\\n') AS info\n",
                "\"\"\"\n",
                ")\n",
                "\n",
                "graph_rag = GraphRAG(llm=llm, retriever=graph_retriever, prompt_template=rag_template)\n",
                "\n",
                "graph_answer = graph_rag.search(q, retriever_config={'top_k': 5}).answer\n",
                "print(\"=== Graph RAG ===\")\n",
                "print(graph_answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "k1f2a3b4",
            "metadata": {},
            "source": [
                "---\n",
                "### Option 3: Hybrid Query\n",
                "\n",
                "Combines **both** retrieval strategies:\n",
                "- Vector retriever provides semantically relevant text chunks\n",
                "- Graph retriever adds structured entity/relationship context\n",
                "\n",
                "The merged context is sent to the LLM in a single prompt for a more comprehensive answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "l2a3b4c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "def hybrid_query(query: str, top_k: int = 5) -> str:\n",
                "    \"\"\"Run both vector and graph retrievers, merge context, and generate a single answer.\"\"\"\n",
                "\n",
                "    # 1. Retrieve context from both sources\n",
                "    vector_context = vector_retriever.search(query_text=query, top_k=top_k)\n",
                "    graph_context = graph_retriever.search(query_text=query, top_k=top_k)\n",
                "\n",
                "    # 2. Format vector context (text chunks)\n",
                "    vector_texts = []\n",
                "    for item in vector_context.items:\n",
                "        text = item.content\n",
                "        if text:\n",
                "            vector_texts.append(text)\n",
                "    vector_section = \"\\n---\\n\".join(vector_texts)\n",
                "\n",
                "    # 3. Format graph context (entity relationships)\n",
                "    graph_texts = []\n",
                "    for item in graph_context.items:\n",
                "        text = item.content\n",
                "        if text:\n",
                "            graph_texts.append(text)\n",
                "    graph_section = \"\\n---\\n\".join(graph_texts)\n",
                "\n",
                "    # 4. Build merged context\n",
                "    merged_context = f\"\"\"== Text Chunks (Semantic Search) ==\n",
                "{vector_section}\n",
                "\n",
                "== Entity Relationships (Graph Traversal) ==\n",
                "{graph_section}\"\"\"\n",
                "\n",
                "    # 5. Build prompt and call LLM directly\n",
                "    prompt = rag_template.format(query_text=query, context=merged_context, examples='')\n",
                "    response = llm.invoke(prompt)\n",
                "    return response.content\n",
                "\n",
                "\n",
                "hybrid_answer = hybrid_query(q)\n",
                "print(\"=== Hybrid Query ===\")\n",
                "print(hybrid_answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "m3b4c5d6",
            "metadata": {},
            "source": [
                "---\n",
                "### Compare Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "n4c5d6e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"VECTOR RAG\")\n",
                "print(\"=\" * 80)\n",
                "print(vector_answer)\n",
                "print()\n",
                "print(\"=\" * 80)\n",
                "print(\"GRAPH RAG\")\n",
                "print(\"=\" * 80)\n",
                "print(graph_answer)\n",
                "print()\n",
                "print(\"=\" * 80)\n",
                "print(\"HYBRID (VECTOR + GRAPH)\")\n",
                "print(\"=\" * 80)\n",
                "print(hybrid_answer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}